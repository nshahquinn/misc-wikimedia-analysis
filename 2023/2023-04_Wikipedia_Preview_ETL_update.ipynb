{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caae2461-83db-4bb8-a0de-2166ad160434",
   "metadata": {},
   "source": [
    "I want to update the Wikipedia Preview ETL job to exclude bot traffic and improve our device classification ([T332960](https://phabricator.wikimedia.org/T332960))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98144945-037c-444c-83b9-90c6b28d2561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wmfdata as wmf\n",
    "from wmfdata.utils import pd_display_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943e6044-6a34-408c-95ea-6e0d9702bbba",
   "metadata": {},
   "source": [
    "First, I get the existing data output for a recent day. I'll make sure the updated job accurately replicates the it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8de6d39c-8d83-450e-b683-2230bd5b17cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"\"\"\n",
    "    SELECT\n",
    "        year,\n",
    "        month,\n",
    "        day,\n",
    "        device_type,\n",
    "        referer_host IN ('bjmoreshet.org', 'richardbevan.co.uk') AS site_had_bots,\n",
    "        SUM(previews) AS previews,\n",
    "        SUM(pageviews) AS pageviews\n",
    "    FROM {table}\n",
    "    WHERE\n",
    "        year = 2023\n",
    "        AND month = 4\n",
    "        AND day = 15\n",
    "    GROUP BY\n",
    "        year,\n",
    "        month,\n",
    "        day,\n",
    "        device_type,\n",
    "        referer_host IN ('bjmoreshet.org', 'richardbevan.co.uk')\n",
    "\"\"\"\n",
    "\n",
    "old = wmf.presto.run(test_query.format(table=\"wmf_product.wikipediapreview_stats\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f088514-4fe1-4f0f-bcf4-8853a6040ed8",
   "metadata": {},
   "source": [
    "I drop the existing test table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd7a0301-5305-4f9a-b8ae-7f6ffb960988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neilpquinn-wmf/.conda/envs/2022-11-30T20.59.36_neilpquinn-wmf/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "wmf.hive.run(\"DROP TABLE nshahquinn.wikipediapreview_stats_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dfd9a5-9ed6-45f3-9751-ce6675e38dc1",
   "metadata": {},
   "source": [
    "And run the updated job for the same day in a non-Jupyter shell using `deploy-oozie-job wikipediapreview_stats --test`.\n",
    "\n",
    "Next, I get the new output from the test table and compare to the old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f88036ed-eb00-4fe4-b927-bbcf45066dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = wmf.presto.run(test_query.format(table=\"nshahquinn.wikipediapreview_stats_test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd79ade6-c8f0-4d5b-ac5d-c9321b19fc8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>device_type</th>\n",
       "      <th>site_had_bots</th>\n",
       "      <th>previews</th>\n",
       "      <th>pageviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>non-touch</td>\n",
       "      <td>False</td>\n",
       "      <td>906</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>touch</td>\n",
       "      <td>False</td>\n",
       "      <td>108</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>non-touch</td>\n",
       "      <td>True</td>\n",
       "      <td>861</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>touch</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>non-touch</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>touch</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day device_type site_had_bots  previews  pageviews\n",
       "5  2023      4   15   non-touch         False       906         31\n",
       "2  2023      4   15       touch         False       108         48\n",
       "4  2023      4   15   non-touch          True       861         10\n",
       "3  2023      4   15       touch          True         3          1\n",
       "0  2023      4   15   non-touch          None         3          4\n",
       "1  2023      4   15       touch          None         0          6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old.sort_values([\"site_had_bots\", \"device_type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfb6df7-a75d-42c3-b29e-5502df2d3465",
   "metadata": {},
   "source": [
    "The new output looks right, matching except for:\n",
    "1. a big loss of non-touch previews on sites which had a lot of bot traffic on our previous check\n",
    "2. some of the \"touch\" traffic being recategorized as \"touchscreen computer\"\n",
    "3. a very small loss of non-touch previews on non-bot-heavy sites and among the traffic without a referrer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfef2deb-21ab-4273-9adb-1499a30e72e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>device_type</th>\n",
       "      <th>site_had_bots</th>\n",
       "      <th>previews</th>\n",
       "      <th>pageviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>non-touch</td>\n",
       "      <td>False</td>\n",
       "      <td>878</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>touch</td>\n",
       "      <td>False</td>\n",
       "      <td>95</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>touchscreen computer</td>\n",
       "      <td>False</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>non-touch</td>\n",
       "      <td>True</td>\n",
       "      <td>191</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>touch</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>non-touch</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>touch</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>touchscreen computer</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day           device_type site_had_bots  previews  pageviews\n",
       "6  2023      4   15             non-touch         False       878         31\n",
       "7  2023      4   15                 touch         False        95         41\n",
       "1  2023      4   15  touchscreen computer         False        13          7\n",
       "2  2023      4   15             non-touch          True       191         10\n",
       "5  2023      4   15                 touch          True         3          1\n",
       "0  2023      4   15             non-touch          None         3          4\n",
       "3  2023      4   15                 touch          None         0          3\n",
       "4  2023      4   15  touchscreen computer          None         0          2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new.sort_values([\"site_had_bots\", \"device_type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93627b9-6ce1-4208-910e-8d69c8a2cfb9",
   "metadata": {},
   "source": [
    "Now, to deploy the new job and backfill using the update. We have source data going back 90 days, so we'll backfill for the past 85 days, starting on 2023-01-23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f97cb118-0f1a-4d46-8a73-66e61a58d4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2023, 1, 23)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Timestamp.today().date() - pd.Timedelta(days=85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842c7269-a2df-467d-b5e7-90b32f2aab38",
   "metadata": {},
   "source": [
    "I kill the existing Oozie job using `oozie job -kill 0149075-220913162928808-oozie-oozi-C`.\n",
    "\n",
    "Next, I backup the existing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e0ce9a4-a43d-41e2-934b-313f905b90be",
   "metadata": {},
   "outputs": [],
   "source": [
    "wmf.spark.run(\"DROP TABLE nshahquinn.wikipediapreview_stats_backup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20bc99ba-0dad-45cd-8070-89839fe75b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/18 02:53:17 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "wmf.spark.run([\n",
    "    \"\"\"\n",
    "    CREATE TABLE nshahquinn.wikipediapreview_stats_backup\n",
    "    LIKE wmf_product.wikipediapreview_stats\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    INSERT INTO nshahquinn.wikipediapreview_stats_backup\n",
    "    SELECT *\n",
    "    FROM wmf_product.wikipediapreview_stats\n",
    "    \"\"\"\n",
    "])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5c4f56-2093-4846-a692-c32c7b6588a2",
   "metadata": {},
   "source": [
    "Now, I replace the existing dataset with a truncated version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e24f7182-564f-4293-a63e-76dbc2fe4e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neilpquinn-wmf/.conda/envs/2022-11-30T20.59.36_neilpquinn-wmf/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/neilpquinn-wmf/.conda/envs/2022-11-30T20.59.36_neilpquinn-wmf/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/neilpquinn-wmf/.conda/envs/2022-11-30T20.59.36_neilpquinn-wmf/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "wmf.hive.run(\"DROP TABLE wmf_product.wikipediapreview_stats\")\n",
    "\n",
    "wmf.hive.run(\"\"\"\n",
    "CREATE EXTERNAL TABLE wmf_product.wikipediapreview_stats (\n",
    "    `pageviews`      bigint  COMMENT 'Number of pageviews shown as a result of a clickthrough from a Wikipedia Preview preview',\n",
    "    `previews`       bigint  COMMENT 'Number of API requests for article preview content made by Wikipedia Preview clients',\n",
    "    `year`           int     COMMENT 'Unpadded year of request',\n",
    "    `month`          int     COMMENT 'Unpadded month of request',\n",
    "    `day`            int     COMMENT 'Unpadded day of request',\n",
    "    `device_type`    string  COMMENT 'Type of device used by the client: touch or non-touch',\n",
    "    `referer_host`   string  COMMENT 'Host from referer parsing',\n",
    "    `continent`      string  COMMENT 'Continent of the accessing agents (maxmind GeoIP database)',\n",
    "    `country_code`   string  COMMENT 'Country iso code of the accessing agents (maxmind GeoIP database)',\n",
    "    `country`        string  COMMENT 'Country (text) of the accessing agents (maxmind GeoIP database)',\n",
    "    `instrumentation_version` int COMMENT 'Version number incremented along with major instrumentation changes'\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY '\\t'\n",
    "LINES TERMINATED BY '\\n'\n",
    "STORED AS TEXTFILE\n",
    "LOCATION 'hdfs://analytics-hadoop//user/analytics-product/wikipediapreview_stats/daily'\n",
    "\"\"\")\n",
    "\n",
    "wmf.hive.run([\n",
    "    \"SET hive.exec.compress.output=true\",\n",
    "    \"SET mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec\",\n",
    "    \"\"\"\n",
    "    INSERT INTO wmf_product.wikipediapreview_stats\n",
    "    SELECT *\n",
    "    FROM nshahquinn.wikipediapreview_stats_backup\n",
    "    WHERE\n",
    "        year < 2023 \n",
    "        OR (year = 2023 AND month = 1 and day < 23)\n",
    "    -- The ORDER BY forces a single output file \n",
    "    ORDER BY\n",
    "        year,\n",
    "        month,\n",
    "        day\n",
    "    LIMIT 10000000\n",
    "    \"\"\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dd6b78-6476-42be-86c1-a2e910d0fffc",
   "metadata": {},
   "source": [
    "Checking that I loaded what I wanted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce90be4a-275b-468d-adbd-2123fb1e0fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neilpquinn-wmf/.conda/envs/2022-11-30T20.59.36_neilpquinn-wmf/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "daily_counts = wmf.hive.run(\"\"\"\n",
    "    SELECT\n",
    "        year,\n",
    "        month,\n",
    "        day,\n",
    "        SUM(previews) AS previews,\n",
    "        SUM(pageviews) AS pageviews\n",
    "    FROM wmf_product.wikipediapreview_stats\n",
    "    GROUP BY\n",
    "        year,\n",
    "        month,\n",
    "        day\n",
    "    ORDER BY \n",
    "        year,\n",
    "        month,\n",
    "        day\n",
    "    LIMIT 10000\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04a272f7-4ceb-4fc5-886a-600e92398eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>previews</th>\n",
       "      <th>pageviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>136</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>186</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day  previews  pageviews\n",
       "0  2020     10   20       136          2\n",
       "1  2020     10   21        44          4\n",
       "2  2020     10   22        18          0\n",
       "3  2020     10   23        72          0\n",
       "4  2020     10   26       186          2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00654668-b5e9-4adc-8d38-9dd18eee9b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>previews</th>\n",
       "      <th>pageviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>1203</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>1259</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>1881</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>1270</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>1837</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     year  month  day  previews  pageviews\n",
       "895  2023      4   13      1203        189\n",
       "896  2023      4   14      1259        110\n",
       "897  2023      4   15      1881        100\n",
       "898  2023      4   16      1270        109\n",
       "899  2023      4   17      1837        110"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/18 03:07:52 WARN UserGroupInformation: Not attempting to re-login since the last re-login was attempted less than 60 seconds before. Last Login=1681787236814\n"
     ]
    }
   ],
   "source": [
    "daily_counts.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac482ad-8082-4658-8e2f-3d45af2be506",
   "metadata": {},
   "source": [
    "Um, no, I didn't get the right data...and now I remember, that, since this is an external table for Hive, dropping the table didn't delete the underlying data files in HDFS. I'm pretty sure I had to make this same mistake and find out during every previous update of this ETL job too..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e74128c0-a7cc-4124-b6d4-f49b553dd40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rwxrwxr-x   3 neilpquinn-wmf    analytics-privatedata-users     504405 2023-04-18 02:59 /user/analytics-product/wikipediapreview_stats/daily/000000_0.gz\n",
      "-rw-r--r--   3 analytics-product hdfs                            650266 2023-04-18 00:44 /user/analytics-product/wikipediapreview_stats/daily/data.gz\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/analytics-product/wikipediapreview_stats/daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9188b769-867c-4c53-90c5-aa49972223d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/18 03:09:47 INFO fs.TrashPolicyDefault: Moved: 'hdfs://analytics-hadoop/user/analytics-product/wikipediapreview_stats/daily/data.gz' to trash at: hdfs://analytics-hadoop/user/neilpquinn-wmf/.Trash/Current/user/analytics-product/wikipediapreview_stats/daily/data.gz\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm /user/analytics-product/wikipediapreview_stats/daily/data.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13c4c39f-f03c-4577-80a2-e172b58b4493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/18 03:10:14 INFO fs.TrashPolicyDefault: Moved: 'hdfs://analytics-hadoop/user/analytics-product/wikipediapreview_stats/daily/000000_0.gz' to trash at: hdfs://analytics-hadoop/user/neilpquinn-wmf/.Trash/Current/user/analytics-product/wikipediapreview_stats/daily/000000_0.gz\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm /user/analytics-product/wikipediapreview_stats/daily/000000_0.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8302f934-5953-4c3c-a2fa-157d3d0666bf",
   "metadata": {},
   "source": [
    "Now, to repeat the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3bd0c88e-3f27-415b-9386-87b48c1d8b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neilpquinn-wmf/.conda/envs/2022-11-30T20.59.36_neilpquinn-wmf/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/neilpquinn-wmf/.conda/envs/2022-11-30T20.59.36_neilpquinn-wmf/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/neilpquinn-wmf/.conda/envs/2022-11-30T20.59.36_neilpquinn-wmf/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "wmf.hive.run(\"DROP TABLE wmf_product.wikipediapreview_stats\")\n",
    "\n",
    "wmf.hive.run(\"\"\"\n",
    "CREATE EXTERNAL TABLE wmf_product.wikipediapreview_stats (\n",
    "    `pageviews`      bigint  COMMENT 'Number of pageviews shown as a result of a clickthrough from a Wikipedia Preview preview',\n",
    "    `previews`       bigint  COMMENT 'Number of API requests for article preview content made by Wikipedia Preview clients',\n",
    "    `year`           int     COMMENT 'Unpadded year of request',\n",
    "    `month`          int     COMMENT 'Unpadded month of request',\n",
    "    `day`            int     COMMENT 'Unpadded day of request',\n",
    "    `device_type`    string  COMMENT 'Type of device used by the client: touch or non-touch',\n",
    "    `referer_host`   string  COMMENT 'Host from referer parsing',\n",
    "    `continent`      string  COMMENT 'Continent of the accessing agents (maxmind GeoIP database)',\n",
    "    `country_code`   string  COMMENT 'Country iso code of the accessing agents (maxmind GeoIP database)',\n",
    "    `country`        string  COMMENT 'Country (text) of the accessing agents (maxmind GeoIP database)',\n",
    "    `instrumentation_version` int COMMENT 'Version number incremented along with major instrumentation changes'\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY '\\t'\n",
    "LINES TERMINATED BY '\\n'\n",
    "STORED AS TEXTFILE\n",
    "LOCATION 'hdfs://analytics-hadoop//user/analytics-product/wikipediapreview_stats/daily'\n",
    "\"\"\")\n",
    "\n",
    "wmf.hive.run([\n",
    "    \"SET hive.exec.compress.output=true\",\n",
    "    \"SET mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec\",\n",
    "    \"\"\"\n",
    "    INSERT INTO wmf_product.wikipediapreview_stats\n",
    "    SELECT *\n",
    "    FROM nshahquinn.wikipediapreview_stats_backup\n",
    "    WHERE\n",
    "        year < 2023 \n",
    "        OR (year = 2023 AND month = 1 and day < 23)\n",
    "    -- The ORDER BY forces a single output file \n",
    "    ORDER BY\n",
    "        year,\n",
    "        month,\n",
    "        day\n",
    "    LIMIT 10000000\n",
    "    \"\"\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1c2daf-17a7-4800-851a-5535f2690caa",
   "metadata": {},
   "source": [
    "And checking again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "84329a5a-39ad-46f5-9da0-a428f460e446",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neilpquinn-wmf/.conda/envs/2022-11-30T20.59.36_neilpquinn-wmf/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "daily_counts = wmf.hive.run(\"\"\"\n",
    "    SELECT\n",
    "        year,\n",
    "        month,\n",
    "        day,\n",
    "        SUM(previews) AS previews,\n",
    "        SUM(pageviews) AS pageviews\n",
    "    FROM wmf_product.wikipediapreview_stats\n",
    "    GROUP BY\n",
    "        year,\n",
    "        month,\n",
    "        day\n",
    "    ORDER BY \n",
    "        year,\n",
    "        month,\n",
    "        day\n",
    "    LIMIT 10000\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a4e87d9-6a01-4e90-b9bf-c646113351a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>previews</th>\n",
       "      <th>pageviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day  previews  pageviews\n",
       "0  2020     10   20        68          1\n",
       "1  2020     10   21        22          2\n",
       "2  2020     10   22         9          0\n",
       "3  2020     10   23        36          0\n",
       "4  2020     10   26        93          1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c2e32882-2006-499a-b051-8e1111de5661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>previews</th>\n",
       "      <th>pageviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>4229</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>6928</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>3875</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>2211</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>1820</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     year  month  day  previews  pageviews\n",
       "810  2023      1   18      4229        134\n",
       "811  2023      1   19      6928        221\n",
       "812  2023      1   20      3875        140\n",
       "813  2023      1   21      2211        121\n",
       "814  2023      1   22      1820         70"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/18 13:43:12 WARN Client: Failed to cleanup staging dir hdfs://analytics-hadoop/user/neilpquinn-wmf/.sparkStaging/application_1678266962370_232108\n",
      "org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/neilpquinn-wmf/.sparkStaging/application_1678266962370_232108. Name node is in safe mode.\n",
      "It was turned on manually. Use \"hdfs dfsadmin -safemode leave\" to turn safe mode off. NamenodeHostName:an-master1001.eqiad.wmnet\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1436)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1423)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:2867)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:1110)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:648)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:498)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1038)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1003)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:931)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1938)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2855)\n",
      "\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\n",
      "\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:1608)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:952)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:949)\n",
      "\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:959)\n",
      "\tat org.apache.spark.deploy.yarn.Client.cleanupStagingDirInternal$1(Client.scala:232)\n",
      "\tat org.apache.spark.deploy.yarn.Client.cleanupStagingDir(Client.scala:241)\n",
      "\tat org.apache.spark.deploy.yarn.Client.monitorApplication(Client.scala:1117)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:117)\n",
      "Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot delete /user/neilpquinn-wmf/.sparkStaging/application_1678266962370_232108. Name node is in safe mode.\n",
      "It was turned on manually. Use \"hdfs dfsadmin -safemode leave\" to turn safe mode off. NamenodeHostName:an-master1001.eqiad.wmnet\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1436)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1423)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:2867)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:1110)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:648)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:498)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1038)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1003)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:931)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1938)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2855)\n",
      "\n",
      "\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1457)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1367)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n",
      "\tat com.sun.proxy.$Proxy14.delete(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.delete(ClientNamenodeProtocolTranslatorPB.java:637)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n",
      "\tat com.sun.proxy.$Proxy15.delete(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:1606)\n",
      "\t... 8 more\n",
      "23/04/18 13:43:12 ERROR YarnClientSchedulerBackend: YARN application has exited unexpectedly with state SUCCEEDED! Check the YARN application logs for more details.\n",
      "23/04/18 13:45:12 ERROR Utils: Uncaught exception in thread YARN application state monitor\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.requestTotalExecutors(CoarseGrainedSchedulerBackend.scala:742)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnSchedulerBackend.stop(YarnSchedulerBackend.scala:114)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.stop(YarnClientSchedulerBackend.scala:168)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:881)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2370)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2069)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1419)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2069)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:124)\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:293)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\t... 9 more\n",
      "23/04/18 13:45:12 ERROR YarnSchedulerBackend$YarnSchedulerEndpoint: Sending RequestExecutors(Map(),Map(),Map(),Set()) to AM was unsuccessful\n",
      "org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from /10.64.21.114:33052 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat scala.util.Failure.recover(Try.scala:234)\n",
      "\tat scala.concurrent.Future.$anonfun$recover$1(Future.scala:395)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.tryFailure(Promise.scala:112)\n",
      "\tat scala.concurrent.Promise.tryFailure$(Promise.scala:112)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:214)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:264)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from /10.64.21.114:33052 in 120 seconds\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:265)\n",
      "\t... 7 more\n",
      "23/04/18 13:45:12 WARN NettyRpcEnv: Ignored failure: org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from /10.64.21.114:33052 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout\n"
     ]
    }
   ],
   "source": [
    "daily_counts.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b56bf9-2b51-412a-8e8f-91f401e2cb78",
   "metadata": {},
   "source": [
    "Much better! (The Hadoop error is due to unrelated maintenance on the cluster). Now, I deploy the updated job:\n",
    "\n",
    "```\n",
    "neilpquinn-wmf@stat1005:~/product_analytics_jobs$ ./deploy-oozie-job wikipediapreview_stats --production\n",
    "The HDFS job directory will be hdfs:///user/analytics-product/jobs/wikipediapreview_stats\n",
    "Removing old job files in the job directory...\n",
    "Creating the job directory...\n",
    "Putting new job files into the job directory...\n",
    "Submitting the job...\n",
    "job: 0174537-220913162928808-oozie-oozi-C\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c36c5a-0a67-4a0a-b60a-aeb49690570e",
   "metadata": {},
   "source": [
    "Well, after letting the job run overnight, I have two problems:\n",
    "1. after running for correctly for the first 60 days, the task for 2023-03-24 failed.\n",
    "2. the data from before the backfill now looks wildly inflated. I think something is causing the old data to get duplicated with every run of the job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ff583a-38dd-469f-bbfa-4c5d5c2d7f5a",
   "metadata": {},
   "source": [
    "Ah! I deleted the old data files, but I forgot that I have to rename the loaded data to `data.gz`, or we get the data duplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c8cdbb29-c28e-45a3-a487-3367eb1e5d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rwxrwxr-x   3 neilpquinn-wmf    analytics-privatedata-users     504405 2023-04-18 03:12 /user/analytics-product/wikipediapreview_stats/daily/000000_0.gz\n",
      "-rw-r--r--   3 analytics-product hdfs                          30356281 2023-04-18 11:38 /user/analytics-product/wikipediapreview_stats/daily/data.gz\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/analytics-product/wikipediapreview_stats/daily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae7f216-7b79-493e-9245-c61a681719bd",
   "metadata": {},
   "source": [
    "I'm pretty sure `000000_0.gz` is the data I loaded in the first place, and it gets re-incorporated into `data.gz` every time the data runs. So I can just delete `data.gz`, rename `000000_0.gz` to `data.gz`, and then re-run the backfill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "da91067d-6145-4548-8d7d-8fc1c322f80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/18 21:34:58 INFO fs.TrashPolicyDefault: Moved: 'hdfs://analytics-hadoop/user/analytics-product/wikipediapreview_stats/daily/data.gz' to trash at: hdfs://analytics-hadoop/user/neilpquinn-wmf/.Trash/Current/user/analytics-product/wikipediapreview_stats/daily/data.gz1681853698079\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm /user/analytics-product/wikipediapreview_stats/daily/data.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d25efcab-d74c-40da-9ce9-c37061ed8ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mv /user/analytics-product/wikipediapreview_stats/daily/000000_0.gz /user/analytics-product/wikipediapreview_stats/daily/data.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "24b8b49c-3b69-480b-a3e7-e865fdc6927a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rwxrwxr-x   3 neilpquinn-wmf analytics-privatedata-users     504405 2023-04-18 03:12 /user/analytics-product/wikipediapreview_stats/daily/data.gz\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/analytics-product/wikipediapreview_stats/daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b9aadc1d-2f5b-476e-b4ba-24d811c948e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/18 21:41:15 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "23/04/18 21:41:15 WARN Utils: Service 'sparkDriver' could not bind on port 12000. Attempting port 12001.\n",
      "23/04/18 21:41:15 WARN Utils: Service 'sparkDriver' could not bind on port 12001. Attempting port 12002.\n",
      "23/04/18 21:41:15 WARN Utils: Service 'sparkDriver' could not bind on port 12002. Attempting port 12003.\n",
      "23/04/18 21:41:15 WARN Utils: Service 'sparkDriver' could not bind on port 12003. Attempting port 12004.\n",
      "23/04/18 21:41:15 WARN Utils: Service 'sparkDriver' could not bind on port 12004. Attempting port 12005.\n",
      "23/04/18 21:41:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/04/18 21:41:15 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/04/18 21:41:15 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/04/18 21:41:15 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "23/04/18 21:41:15 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "23/04/18 21:41:21 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13000. Attempting port 13001.\n",
      "23/04/18 21:41:21 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13001. Attempting port 13002.\n",
      "23/04/18 21:41:21 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13002. Attempting port 13003.\n",
      "23/04/18 21:41:21 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13003. Attempting port 13004.\n",
      "23/04/18 21:41:21 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13004. Attempting port 13005.\n",
      "23/04/18 21:41:21 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://stat1005.eqiad.wmnet:4045\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>wmfdata-yarn-regular</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2b0fd64880>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmf.spark.create_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2883ca-2b36-4caf-bff6-af3eac495bec",
   "metadata": {},
   "source": [
    "The data in the table looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b40655d7-30b9-4e1f-a9f3-635c4d9ff1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "daily_counts = wmf.spark.run(\"\"\"\n",
    "    SELECT\n",
    "        year,\n",
    "        month,\n",
    "        day,\n",
    "        SUM(previews) AS previews,\n",
    "        SUM(pageviews) AS pageviews\n",
    "    FROM wmf_product.wikipediapreview_stats\n",
    "    GROUP BY\n",
    "        year,\n",
    "        month,\n",
    "        day\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "897c0c55-f2d0-449a-945f-aa5d118d3fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>previews</th>\n",
       "      <th>pageviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     year  month  day  previews  pageviews\n",
       "578  2020     10   20        68          1\n",
       "732  2020     10   21        22          2\n",
       "247  2020     10   22         9          0\n",
       "546  2020     10   23        36          0\n",
       "801  2020     10   26        93          1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_counts = daily_counts.sort_values([\"year\", \"month\", \"day\"])\n",
    "daily_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c0680a46-64a3-4aec-9a68-c9e51cff9b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>previews</th>\n",
       "      <th>pageviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>4229</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>6928</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>3875</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>2211</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>1820</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     year  month  day  previews  pageviews\n",
       "776  2023      1   18      4229        134\n",
       "103  2023      1   19      6928        221\n",
       "64   2023      1   20      3875        140\n",
       "352  2023      1   21      2211        121\n",
       "690  2023      1   22      1820         70"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_counts.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bf31c2-5def-4bcd-a010-c9bc4b29ffc5",
   "metadata": {},
   "source": [
    "I kill the Oozie job using `oozie job -kill 0174537-220913162928808-oozie-oozi-C` and deploy the updated job:\n",
    "```\n",
    "neilpquinn-wmf@stat1005:~/product_analytics_jobs$ ./deploy-oozie-job wikipediapreview_stats --production\n",
    "The HDFS job directory will be hdfs:///user/analytics-product/jobs/wikipediapreview_stats\n",
    "Removing old job files in the job directory...\n",
    "Creating the job directory...\n",
    "Putting new job files into the job directory...\n",
    "Submitting the job...\n",
    "job: 0174858-220913162928808-oozie-oozi-C\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a535e37-84f1-4ee5-950e-492eb1a675cd",
   "metadata": {},
   "source": [
    "The first two daily tasks have succeeded, and everything looks good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "82dbec2f-5ac1-4f6b-b038-afcf2afe3c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   3 analytics-product hdfs     509120 2023-04-18 21:55 /user/analytics-product/wikipediapreview_stats/daily/data.gz\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/analytics-product/wikipediapreview_stats/daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "33579ba2-6269-439a-9680-e7ccbf7870bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>previews</th>\n",
       "      <th>pageviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>4229</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>6928</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>3875</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>2211</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>1820</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>1785</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>1563</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day  previews  pageviews\n",
       "6  2023      1   18      4229        134\n",
       "1  2023      1   19      6928        221\n",
       "0  2023      1   20      3875        140\n",
       "2  2023      1   21      2211        121\n",
       "5  2023      1   22      1820         70\n",
       "3  2023      1   23      1785         92\n",
       "4  2023      1   24      1563         76"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmf.spark.run(\"\"\"\n",
    "    SELECT\n",
    "        year,\n",
    "        month,\n",
    "        day,\n",
    "        SUM(previews) AS previews,\n",
    "        SUM(pageviews) AS pageviews\n",
    "    FROM wmf_product.wikipediapreview_stats\n",
    "    WHERE\n",
    "        year = 2023\n",
    "        AND month = 1\n",
    "        AND day >= 18\n",
    "    GROUP BY\n",
    "        year,\n",
    "        month,\n",
    "        day\n",
    "\"\"\").sort_values([\"year\", \"month\", \"day\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
